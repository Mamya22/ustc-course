@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}

@article{10123038,
  author   = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Multimodal Learning With Transformers: A Survey},
  year     = {2023},
  volume   = {45},
  number   = {10},
  pages    = {12113-12132},
  keywords = {Transformers;Task analysis;Surveys;Visualization;Taxonomy;Mathematical models;Data models;Multimodal learning;transformer;introductory;taxonomy;deep learning;machine learning},
  doi      = {10.1109/TPAMI.2023.3275156}
}

@inproceedings{chen2022visualgpt,
  title     = {Visualgpt: Data-efficient adaptation of pretrained language models for image captioning},
  author    = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {18030--18040},
  year      = {2022}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}

@inproceedings{10.1007/978-3-030-58577-8_7,
  author    = {Chen, Yen-Chun
               and Li, Linjie
               and Yu, Licheng
               and El Kholy, Ahmed
               and Ahmed, Faisal
               and Gan, Zhe
               and Cheng, Yu
               and Liu, Jingjing},
  editor    = {Vedaldi, Andrea
               and Bischof, Horst
               and Brox, Thomas
               and Frahm, Jan-Michael},
  title     = {UNITER: UNiversal Image-TExt Representation Learning},
  booktitle = {Computer Vision -- ECCV 2020},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {104--120},
  abstract  = {Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR{\$}{\$}^2{\$}{\$}2(Code is available at https://github.com/ChenRocks/UNITER.).},
  isbn      = {978-3-030-58577-8}
}

@article{lu2019vilbert,
  title   = {Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author  = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}


@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PMLR}
}

@inproceedings{2020UNITER,
  title     = {UNITER: UNiversal Image-TExt Representation Learning},
  author    = { Chen, Yen Chun  and  Li, Linjie  and  Yu, Licheng  and  El Kholy, Ahmed  and  Ahmed, Faisal  and  Gan, Zhe  and  Cheng, Yu  and  Liu, Jingjing },
  booktitle = {European Conference on Computer Vision},
  year      = {2020}
}

@article{tan2019lxmert,
  title   = {Lxmert: Learning cross-modality encoder representations from transformers},
  author  = {Tan, Hao and Bansal, Mohit},
  journal = {arXiv preprint arXiv:1908.07490},
  year    = {2019}
}